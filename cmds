# start riffusion server
CUDA_VISIBLE_DEVICES=1 python -m riffusion.server --host 127.0.0.1 --port 8080

# run riffusion inference
curl -X POST http://127.0.0.1:8080/run_inference/ -H "Content-Type: application/json" -d '{"start":{"prompt":"","seed":42,"denoising":0.75,"guidance":7.0},"end":{"prompt":"","seed":123,"denoising":0.75,"guidance":7.0},"alpha":0.5,"num_inference_steps":50,"seed_image_id":"mask_beat_lines_80"}'

# run riffusion inference simple
curl -X POST http://127.0.0.1:8080/run_inference/ -H "Content-Type: application/json" -d '{"start":{"prompt":"","seed":42,"denoising":0.75,"guidance":7.0},"num_inference_steps":50,"seed_image_id":"mask_beat_lines_80"}'

# run riffusion inference simple with mask image
CUDA_VISIBLE_DEVICES=1 curl -X POST http://127.0.0.1:8080/run_inference/ \
  -H "Content-Type: application/json" \
  -d '{
    "start": {
      "prompt": "",
      "seed": 42,
      "denoising": 0.2,
      "guidance": 0
    },
    "num_inference_steps": 50,
    "seed_image_id": "1",
    "mask_image_id": "Chopper_egdb_1_spectrogram_image",
    "alpha":0,
    "end": {
      "prompt": "",
      "seed": 123,
      "denoising": 0.2,
      "guidance": 0
    }
  }'


# base64 decode (w/ riffusion output json file)
python3 decode_audio_from_base64.py --json_file_path   --output_wav_path ../results/riffusion_test/

# dataset paths
/mnt/gestalt/home/ytsrt/EGDB-Large-Subset

/mnt/gestalt/home/ytsrt/reaMIXed

# run styleid inference on EGDB content and tone transfer
CUDA_VISIBLE_DEVICES=1 python3 styleid_inference.py --content_audio /home/mku666/riffusion-hobby/stable_audio_api/sample_data/fx_data/EGDB-Large-Subset/AudioDI/DI_1/1.wav --style_audio /home/mku666/riffusion-hobby/stable_audio_api/sample_data/fx_data/EGDB-Large-Subset/Tone/Chopper/DI_1/1.wav --output_path ./egdb_di_1_chopper_style_output.wav --prompt_start "" --prompt_end ""

# run riffusion styleid inference on MusicTI dataset
CUDA_VISIBLE_DEVICES=1 python3 styleid_inference.py --content_audio /mnt/gestalt/home/mku666/musicTI_audios/content/piano/piano1.wav --style_audio /mnt/gestalt/home/mku666/musicTI_audios/timbre/accordion/accordion1.wav --output_path ./piano_to_accordion_style_output.wav --prompt_start "" --prompt_end "" --start_step 200 --no_adain_init --attention_op_type 1

# run stableaudio styleid inference on MusicTI dataset
CUDA_VISIBLE_DEVICES=1 python styleid_inference_stableaudio.py --content_audio /mnt/gestalt/home/mku666/musicTI_audios/content/piano/piano1.wav --style_audio /mnt/gestalt/home/mku666/musicTI_audios/timbre/accordion/accordion1.wav --output_path ./piano_to_accordion_style_output.wav --gamma 0.8 --T 1.2 --start_step 45 --prompt "" --num_inference_steps 100

# visualize patch embedding overlaying on attention heatmap of riffusion
CUDA_VISIBLE_DEVICES=1 python3 riffusion_heatmap_patch_visualiztion.py --audio_path /mnt/gestalt/home/mku666/musicTI_audios/content/piano/piano1.wav --time_step 5 --attention_layer 6 --query_token 33 --num_inference_steps 30

# plot spectrogram
python3 spectrogram_plot.py --style_path /mnt/gestalt/home/mku666/musicTI_audios/content/piano/piano1.wav --content_path /mnt/gestalt/home/mku666/musicTI_audios/timbre/accordion/accordion1.wav --output_path /home/mku666/riffusion-hobby/piano_to_accordion_style_output.wav

# visualize attention heatmap of riffusion
python riffusion_activation_visualization.py \
    --audio_path /mnt/gestalt/home/mku666/musicTI_audios/content/piano/piano1.wav \
    --time_step 20 \
    --attention_layer "6,7,8,9,10,11" \
    --head 0 \
    --prompt "electronic beats" \
    --num_inference_steps 30 \
    --seed 42

# stable audio runs
CUDA_VISIBLE_DEVICES=0 python styleid_inference_stableaudio.py     --content_audio /mnt/gestalt/home/mku666/musicTI_audios/content/piano/piano1.wav     --style_audio  /mnt/gestalt/home/mku666/musicTI_audios/timbre/accordion/accordion1.wav     --output_path ./results/piano_in_guitar_style.wav     --gamma 0.7     --T 1.3     --num_inference_steps 50