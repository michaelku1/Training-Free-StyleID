{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## git clone project and install independencies"
      ],
      "metadata": {
        "id": "PWUm1wMSozLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# cd to current directory\n",
        "%cd /content/drive/MyDrive/riffusion_project/\n",
        "\n",
        "# clone project\n",
        "!git clone --depth 1 https://github.com/michaelku1/Training-Free-StyleID.git\n",
        "# install environment\n",
        "!curl -L https://repo.anaconda.com/miniconda/Miniconda3-py39_4.12.0-Linux-x86_64.sh -o miniconda.sh\n",
        "!chmod +x miniconda.sh\n",
        "!sh miniconda.sh -b -p /content/miniconda\n",
        "!/content/miniconda/bin/pip install -r requirements.txt\n",
        "!/content/miniconda/bin/pip install --upgrade ipython ipykernel\n",
        "\n",
        "%cd ./Training-Free-StyleID\n",
        "# mitigate ModuleNotFoundError: No module named 'dacite'\n",
        "! pip install dacite flask_cors argh"
      ],
      "metadata": {
        "id": "RKkPfcgwoxaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run project"
      ],
      "metadata": {
        "id": "lzjUBPmHon4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Flask server that serves the riffusion model as an API.\n",
        "\"\"\"\n",
        "\n",
        "import dataclasses\n",
        "import io\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "import typing as T\n",
        "from pathlib import Path\n",
        "\n",
        "import dacite\n",
        "import flask\n",
        "import PIL\n",
        "import torch\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Fix CUDA linear algebra backend to avoid cusolver errors\n",
        "torch.backends.cuda.preferred_linalg_library('magma')\n",
        "\n",
        "# NOTE original riffusion pipeline\n",
        "from riffusion.riffusion_pipeline import RiffusionPipeline\n",
        "from riffusion.datatypes import InferenceInput, InferenceOutput\n",
        "\n",
        "from riffusion.spectrogram_image_converter import SpectrogramImageConverter\n",
        "from riffusion.spectrogram_params import SpectrogramParams\n",
        "\n",
        "from riffusion.util import base64_util\n",
        "\n",
        "# from flask_ngrok import run_with_ngrok\n",
        "NGROK_AUTH_TOKEN = \"32MmrpMI4sZN558sIugyRuhDgDg_5AdY64F9xihYgNZZfyHJL\"\n",
        "\n",
        "# Flask app with CORS\n",
        "app = flask.Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "\n",
        "# Create a logger object\n",
        "logger = logging.getLogger(\"my_server\")\n",
        "logger.setLevel(logging.DEBUG)\n",
        "\n",
        "# Log at the INFO level to both stdout and disk\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.FileHandler(\"server.log\"))\n",
        "\n",
        "# Create a file handler to write logs to a file\n",
        "file_handler = logging.FileHandler(\"server.log\")\n",
        "file_handler.setLevel(logging.DEBUG)\n",
        "\n",
        "# set format\n",
        "formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
        "\n",
        "# initalise file handler\n",
        "file_handler.setFormatter(formatter)\n",
        "\n",
        "# initalise console handler\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.INFO)\n",
        "console_handler.setFormatter(formatter)\n",
        "\n",
        "# Add handlers to the logger\n",
        "logger.addHandler(file_handler)\n",
        "\n",
        "# Global variable for the model pipeline\n",
        "PIPELINE: T.Optional[RiffusionPipeline] = None\n",
        "\n",
        "# set auth token for free n_grok usage\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)"
      ],
      "metadata": {
        "id": "krqMLotxn-NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_request(\n",
        "    inputs: InferenceInput,\n",
        "    pipeline: RiffusionPipeline,\n",
        ") -> T.Union[str, T.Tuple[str, int]]:\n",
        "    \"\"\"\n",
        "    Does all the heavy lifting of the request.\n",
        "\n",
        "    Args:\n",
        "        inputs: The input dataclass\n",
        "        pipeline: The riffusion model pipeline\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the seed image by ID\n",
        "    init_image_path = Path(f\"{inputs.seed_image_path}.png\")\n",
        "\n",
        "    print(\"######################### input image path: \", init_image_path)\n",
        "\n",
        "    if not init_image_path.is_file():\n",
        "        return f\"Invalid seed image: {inputs.seed_image_path}\", 400\n",
        "    init_image = PIL.Image.open(str(init_image_path)).convert(\"RGB\")\n",
        "\n",
        "    # Load the mask image by ID\n",
        "    mask_image: T.Optional[PIL.Image.Image] = None\n",
        "\n",
        "    # NOTE pass mask image here\n",
        "    # mask_image = PIL.Image.open(\"...png\").convert(\"RGB\")\n",
        "    if inputs.mask_image_path:\n",
        "        mask_image_path = Path(f\"{inputs.mask_image_path}.png\")\n",
        "        if not mask_image_path.is_file():\n",
        "            return f\"Invalid mask image: {inputs.mask_image_path}\", 400\n",
        "        mask_image = PIL.Image.open(str(mask_image_path)).convert(\"RGB\")\n",
        "\n",
        "    print(\"inputs:\", inputs)\n",
        "    print(\"init_image\", init_image)\n",
        "    print(\"mask_image\", mask_image)\n",
        "\n",
        "    # Execute the model to get the spectrogram image\n",
        "    image = pipeline.riffuse(\n",
        "        inputs,\n",
        "        init_image=init_image,\n",
        "        mask_image=mask_image,\n",
        "    )\n",
        "\n",
        "    # TODO(hayk): Change the frequency range to [20, 20k] once the model is retrained\n",
        "    params = SpectrogramParams(\n",
        "        min_frequency=0,\n",
        "        max_frequency=10000,\n",
        "    )\n",
        "\n",
        "    # Reconstruct audio from the image\n",
        "    # TODO(hayk): It may help performance a bit to cache this object\n",
        "    # Use CPU for audio processing to avoid CUDA solver issues\n",
        "    converter = SpectrogramImageConverter(params=params, device=\"cpu\")\n",
        "\n",
        "    # NOTE 轉回 audio signal\n",
        "    segment = converter.audio_from_spectrogram_image(\n",
        "        image,\n",
        "        apply_filters=True,\n",
        "    )\n",
        "\n",
        "    # Export audio to MP3 bytes\n",
        "    mp3_bytes = io.BytesIO()\n",
        "    segment.export(mp3_bytes, format=\"mp3\")\n",
        "    mp3_bytes.seek(0)\n",
        "\n",
        "    # Export image to JPEG bytes\n",
        "    image_bytes = io.BytesIO()\n",
        "    image.save(image_bytes, exif=image.getexif(), format=\"JPEG\")\n",
        "    image_bytes.seek(0)\n",
        "\n",
        "    # Assemble the output dataclass\n",
        "    output = InferenceOutput(\n",
        "        image=\"data:image/jpeg;base64,\" + base64_util.encode(image_bytes),\n",
        "        audio=\"data:audio/mpeg;base64,\" + base64_util.encode(mp3_bytes),\n",
        "        duration_s=segment.duration_seconds,\n",
        "    )\n",
        "\n",
        "    # release memory\n",
        "    import gc\n",
        "    del image, mask_image, init_image  # delete big tensors\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()  # free cached memory\n",
        "    torch.cuda.ipc_collect()  # (optional) reclaim inter-process memory\n",
        "\n",
        "    output_name = f\"{''.join(inputs.seed_image_path.split('/')[-2:])}_to_{''.join(inputs.mask_image_path.split('/')[-2:])}\"\n",
        "\n",
        "    with open(f\"{inputs.output_path}/{output_name}.json\", \"w\") as f:\n",
        "        json.dump(dataclasses.asdict(output), f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"output json path:\", output_name, flush=True)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "bQujQzIjo3eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Where built-in seed images are stored\n",
        "# import traceback\n",
        "# def run_app_background(*args, **kwargs):\n",
        "#     try:\n",
        "#         # Your existing Flask + ngrok code\n",
        "#         global PIPELINE\n",
        "\n",
        "#         import logging, sys\n",
        "#         logging.basicConfig(\n",
        "#             level=logging.DEBUG,\n",
        "#             format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "#             handlers=[logging.StreamHandler(sys.stdout)]\n",
        "#         )\n",
        "#         app.logger.setLevel(logging.DEBUG)\n",
        "\n",
        "#         app.logger.info(\"Loading RiffusionPipeline...\")\n",
        "#         PIPELINE = RiffusionPipeline.load_checkpoint(\n",
        "#             checkpoint=kwargs.get(\"checkpoint\", \"riffusion/riffusion-model-v1\"),\n",
        "#             use_traced_unet=not kwargs.get(\"no_traced_unet\", False),\n",
        "#             device=kwargs.get(\"device\", \"cuda\")\n",
        "#         )\n",
        "#         app.logger.info(\"Pipeline loaded successfully!\")\n",
        "\n",
        "#         public_url = ngrok.connect(kwargs.get(\"port\", 5000))\n",
        "#         print(f\" * ngrok tunnel URL: {public_url}\", flush=True)\n",
        "\n",
        "#         app.logger.info(f\"Starting Flask server on port {kwargs.get('port', 5000)}...\")\n",
        "#         app.run(port=kwargs.get(\"port\", 5000), debug=kwargs.get(\"debug\", True), use_reloader=False)\n",
        "\n",
        "#     except Exception:\n",
        "#         print(\"Exception in background thread:\", flush=True)\n",
        "#         traceback.print_exc()\n",
        "\n",
        "def run_app(\n",
        "    *,\n",
        "    checkpoint: str = \"riffusion/riffusion-model-v1\",\n",
        "    no_traced_unet: bool = False,\n",
        "    device: str = \"cuda\",\n",
        "    port: int = 5000,\n",
        "    debug: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Run a Flask API that serves the given riffusion model checkpoint\n",
        "    and exposes it via ngrok.\n",
        "    \"\"\"\n",
        "    global PIPELINE\n",
        "\n",
        "    # Initialize the model\n",
        "    PIPELINE = RiffusionPipeline.load_checkpoint(\n",
        "        checkpoint=checkpoint,\n",
        "        use_traced_unet=not no_traced_unet,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    # Set debug mode\n",
        "    app.debug = debug\n",
        "\n",
        "    # Start ngrok tunnel\n",
        "    public_url = ngrok.connect(port)\n",
        "    print(f\" * ngrok tunnel URL: {public_url}\", flush=True)\n",
        "\n",
        "    # Start Flask server\n",
        "    app.run(port=port)\n",
        "\n",
        "\n",
        "@app.route(\"/run_inference/\", methods=[\"POST\"])\n",
        "def run_inference():\n",
        "    \"\"\"\n",
        "    Execute the riffusion model as an API.\n",
        "\n",
        "    Inputs:\n",
        "        Serialized JSON of the InferenceInput dataclass\n",
        "\n",
        "    Returns:\n",
        "        Serialized JSON of the InferenceOutput dataclass\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Parse the payload as JSON\n",
        "    json_data = json.loads(flask.request.data)\n",
        "\n",
        "    # Log the request\n",
        "    logging.info(json_data)\n",
        "\n",
        "    # Parse an InferenceInput dataclass from the payload\n",
        "    try:\n",
        "        inputs = dacite.from_dict(InferenceInput, json_data)\n",
        "    except dacite.exceptions.WrongTypeError as exception:\n",
        "        logging.info(json_data)\n",
        "        return str(exception), 400\n",
        "    except dacite.exceptions.MissingValueError as exception:\n",
        "        logging.info(json_data)\n",
        "        return str(exception), 400\n",
        "\n",
        "    # NOTE\n",
        "    response = compute_request(\n",
        "        inputs=inputs,\n",
        "        pipeline=PIPELINE,\n",
        "    )\n",
        "\n",
        "    # Log the total time\n",
        "    logging.info(f\"Request took {time.time() - start_time:.2f} s\")\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "# @app.route(\"/run_inference/\", methods=[\"POST\"])\n",
        "# def run_inference():\n",
        "#     \"\"\"\n",
        "#     Execute the riffusion model as an API.\n",
        "\n",
        "#     Inputs:\n",
        "#         Serialized JSON of the InferenceInput dataclass\n",
        "\n",
        "#     Returns:\n",
        "#         Serialized JSON of the InferenceOutput dataclass\n",
        "#     \"\"\"\n",
        "#     start_time = time.time()\n",
        "\n",
        "#     # Parse the payload as JSON\n",
        "#     json_data = json.loads(flask.request.data)\n",
        "\n",
        "#     # Log the request\n",
        "#     logging.info(json_data)\n",
        "\n",
        "#     # Parse an InferenceInput dataclass from the payload\n",
        "#     try:\n",
        "#         inputs = dacite.from_dict(InferenceInput, json_data)\n",
        "#     except dacite.exceptions.WrongTypeError as exception:\n",
        "#         logging.info(json_data)\n",
        "#         return str(exception), 400\n",
        "#     except dacite.exceptions.MissingValueError as exception:\n",
        "#         logging.info(json_data)\n",
        "#         return str(exception), 400\n",
        "\n",
        "#     # NOTE\n",
        "#     response = compute_request(\n",
        "#         inputs=inputs,\n",
        "#         pipeline=PIPELINE,\n",
        "#     )\n",
        "\n",
        "#     # Log the total time\n",
        "#     logging.info(f\"Request took {time.time() - start_time:.2f} s\")\n",
        "\n",
        "#     return response\n",
        "\n",
        "def start_server():\n",
        "  run_app()"
      ],
      "metadata": {
        "id": "Z_40zTgOo3fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set to background thread\n",
        "import threading\n",
        "import time\n",
        "threading.Thread(target=start_server, daemon=True).start()\n",
        "\n",
        "# Give server time to start\n",
        "time.sleep(5) # may need longer startup"
      ],
      "metadata": {
        "id": "_vx3uFmZo3ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if the server is running (keep checking, if this does not show anything then it means model is still uploading)\n",
        "!lsof -i:5000"
      ],
      "metadata": {
        "id": "hhExJkFQo_Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## simple experiment setup"
      ],
      "metadata": {
        "id": "cDU-NnbGpM2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run inference\n",
        "CUDA_DEVICE=1\n",
        "START_SEED=42\n",
        "END_SEED=123\n",
        "\n",
        "DENOISING=0.2\n",
        "GUIDANCE=0.2\n",
        "ALPHA=0\n",
        "STEPS=100\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/Training-Free-StyleID/results/audio\"\n",
        "# SEED_IMAGE_PATH=\"/content/drive/MyDrive/Training-Free-StyleID/results/riffusion_seed_mask_images/accordian123/1\"\n",
        "MASK_IMAGE_PATH=\"/content/drive/MyDrive/Training-Free-StyleID/results/riffusion_seed_mask_images/EGDB_DI_1/chopper/1\"\n",
        "SEED_IMAGE_PATH = \"/content/drive/MyDrive/Training-Free-StyleID/results/riffusion_seed_mask_images/EGDB_DI_1/clean/2\"\n",
        "\n",
        "# Run curl command\n",
        "# !CUDA_VISIBLE_DEVICES=\"$CUDA_DEVICE\" curl -X POST http://127.0.0.1:5000/run_inference/ -H \"Content-Type: application/json\" -d '{\"start\":{\"prompt\":\"\",\"seed\":'\"$START_SEED\"',\"denoising\":'\"$DENOISING\"',\"guidance\":'\"$GUIDANCE\"'},\"num_inference_steps\":'\"$STEPS\"',\"seed_image_path\":\"'\"$SEED_IMAGE_PATH\"'\",\"mask_image_path\":\"'\"$MASK_IMAGE_PATH\"'\",\"alpha\":'\"$ALPHA\"',\"end\":{\"prompt\":\"\",\"seed\":'\"$END_SEED\"',\"denoising\":'\"$DENOISING\"',\"guidance\":'\"$GUIDANCE\"', \"output_path\": '\"$OUTPUT_PATH\"'}}'"
      ],
      "metadata": {
        "id": "Y0hmGCR0o_Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ignore TypeError: The view function did not return a valid response. The return type must be a string, dict, list, tuple with headers or status, Response instance, or WSGI callable, but it was a InferenceOutput.\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "data = {\n",
        "    \"start\": {\"prompt\": \"\", \"seed\": START_SEED, \"denoising\": DENOISING, \"guidance\": GUIDANCE},\n",
        "    \"num_inference_steps\": STEPS,\n",
        "    \"seed_image_path\": SEED_IMAGE_PATH,\n",
        "    \"mask_image_path\": MASK_IMAGE_PATH,\n",
        "    \"alpha\": ALPHA,\n",
        "    \"end\": {\"prompt\": \"\", \"seed\": END_SEED, \"denoising\": DENOISING, \"guidance\": GUIDANCE},\n",
        "    \"output_path\": OUTPUT_PATH,\n",
        "}\n",
        "\n",
        "try:\n",
        "    response = requests.post(\"http://127.0.0.1:5000/run_inference/\", json=data)\n",
        "    logger.info(f\"Response status code: {response.status_code}\")\n",
        "    logger.info(f\"Response text: {response.text[:500]}\")  # limit output to first 500 chars\n",
        "except Exception as e:\n",
        "    logger.error(f\"Request failed: {e}\")"
      ],
      "metadata": {
        "id": "AIKNTK73o_NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o6ECTPqMpFMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eWUU0hS6pFOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## batch experiment setup"
      ],
      "metadata": {
        "id": "Dt9EEhQJpRWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# denoising strength and guidance scale\n",
        "DENOISING = [0.2, 0.3, 0.4, 0.5]\n",
        "GUIDANCE = [0.2, 0.3, 0.4, 0.5]\n",
        "# ALPHA = [0.2, 0.3, 0.4, 0.5]\n",
        "\n",
        "import itertools\n",
        "import requests\n",
        "import json\n",
        "from copy import deepcopy\n",
        "\n",
        "# Your other constants (replace with actual values)\n",
        "START_SEED=42\n",
        "END_SEED=123\n",
        "STEPS = 50\n",
        "MASK_IMAGE_PATH=\"/content/drive/MyDrive/Training-Free-StyleID/results/riffusion_seed_mask_images/EGDB_DI_1/chopper/1\"\n",
        "SEED_IMAGE_PATH = \"/content/drive/MyDrive/Training-Free-StyleID/results/riffusion_seed_mask_images/EGDB_DI_1/clean/2\"\n",
        "ALPHA = 0.5\n",
        "OUTPUT_PATH = \"path/to/output\"\n",
        "API_ENDPOINT = \"http://127.0.0.1:5000/run_inference/\"  # Replace with your actual endpoint\n",
        "\n",
        "# Base data template\n",
        "base_data = {\n",
        "    \"start\": {\"prompt\": \"\", \"seed\": START_SEED},\n",
        "    \"num_inference_steps\": STEPS,\n",
        "    \"seed_image_path\": SEED_IMAGE_PATH,\n",
        "    \"mask_image_path\": MASK_IMAGE_PATH,\n",
        "    \"alpha\": ALPHA,\n",
        "    \"end\": {\"prompt\": \"\", \"seed\": END_SEED},\n",
        "    \"output_path\": OUTPUT_PATH,\n",
        "}\n",
        "\n",
        "def safe_json_parse(response):\n",
        "    \"\"\"Safely parse JSON response with error handling.\"\"\"\n",
        "    try:\n",
        "        return response.json()\n",
        "    except json.JSONDecodeError:\n",
        "        # Return the raw text if JSON parsing fails\n",
        "        return {\"raw_response\": response.text, \"content_type\": response.headers.get('content-type', 'unknown')}\n",
        "\n",
        "def generate_parameter_combinations():\n",
        "    \"\"\"Generate all combinations of denoising and guidance parameters.\"\"\"\n",
        "    combinations = []\n",
        "    for start_denoising, start_guidance, end_denoising, end_guidance in itertools.product(\n",
        "        DENOISING, GUIDANCE, DENOISING, GUIDANCE\n",
        "    ):\n",
        "        combo = {\n",
        "            'start_denoising': start_denoising,\n",
        "            'start_guidance': start_guidance,\n",
        "            'end_denoising': end_denoising,\n",
        "            'end_guidance': end_guidance\n",
        "        }\n",
        "        combinations.append(combo)\n",
        "    return combinations\n",
        "\n",
        "def create_request_data(combo, experiment_id):\n",
        "    \"\"\"Create request data for a specific parameter combination.\"\"\"\n",
        "    data = deepcopy(base_data)\n",
        "\n",
        "    # Add the parameter combinations to start and end\n",
        "    data['start']['denoising'] = combo['start_denoising']\n",
        "    data['start']['guidance'] = combo['start_guidance']\n",
        "    data['end']['denoising'] = combo['end_denoising']\n",
        "    data['end']['guidance'] = combo['end_guidance']\n",
        "\n",
        "    # Optionally modify output path to include experiment details\n",
        "    data['output_path'] = f\"{OUTPUT_PATH}/exp_{experiment_id}_sd{combo['start_denoising']}_sg{combo['start_guidance']}_ed{combo['end_denoising']}_eg{combo['end_guidance']}\"\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def save_results(results, filename=\"./experiment_results.json\"):\n",
        "    \"\"\"Save experiment results to a JSON file.\"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(f\"Results saved to {filename}\")\n",
        "\n",
        "# Alternative: If you want the same denoising/guidance for both start and end\n",
        "def generate_simple_combinations():\n",
        "    \"\"\"Generate combinations where start and end use the same parameters.\"\"\"\n",
        "    combinations = []\n",
        "    for denoising, guidance in itertools.product(DENOISING, GUIDANCE):\n",
        "        combo = {\n",
        "            'denoising': denoising,\n",
        "            'guidance': guidance\n",
        "        }\n",
        "        combinations.append(combo)\n",
        "    return combinations\n",
        "\n",
        "def create_simple_request_data(combo, experiment_id):\n",
        "    \"\"\"Create request data with same parameters for start and end.\"\"\"\n",
        "    data = deepcopy(base_data)\n",
        "\n",
        "    # Use same parameters for both start and end\n",
        "    data['start']['denoising'] = combo['denoising']\n",
        "    data['start']['guidance'] = combo['guidance']\n",
        "    data['end']['denoising'] = combo['denoising']\n",
        "    data['end']['guidance'] = combo['guidance']\n",
        "\n",
        "    data['output_path'] = f\"{OUTPUT_PATH}/exp_{experiment_id}_d{combo['denoising']}_g{combo['guidance']}\"\n",
        "\n",
        "    return data\n",
        "\n",
        "def test_api_endpoint():\n",
        "    \"\"\"Test the API endpoint before running experiments.\"\"\"\n",
        "    print(f\"Testing API endpoint: {API_ENDPOINT}\")\n",
        "\n",
        "    try:\n",
        "        # Try a simple GET request first\n",
        "        response = requests.get(API_ENDPOINT, timeout=10)\n",
        "        print(f\"GET response: {response.status_code}\")\n",
        "        print(f\"Content-Type: {response.headers.get('content-type', 'unknown')}\")\n",
        "        print(f\"Response preview: {response.text[:200]}...\")\n",
        "\n",
        "        # Try a POST with minimal data\n",
        "        test_data = {\"test\": \"connection\"}\n",
        "        response = requests.post(\n",
        "            API_ENDPOINT,\n",
        "            json=test_data,\n",
        "            headers={'Content-Type': 'application/json'},\n",
        "            timeout=10\n",
        "        )\n",
        "        print(f\"POST response: {response.status_code}\")\n",
        "        print(f\"Content-Type: {response.headers.get('content-type', 'unknown')}\")\n",
        "        print(f\"Response preview: {response.text[:200]}...\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"API endpoint test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def load_experiments(json_file_path):\n",
        "    \"\"\"Load experiments from JSON file\"\"\"\n",
        "    try:\n",
        "        with open(json_file_path, 'r') as file:\n",
        "            experiments = json.load(file)\n",
        "        return experiments\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"JSON file not found: {json_file_path}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError as e:\n",
        "        logger.error(f\"Error decoding JSON: {e}\")\n",
        "        return None\n",
        "\n",
        "def run_experiment(experiment):\n",
        "    \"\"\"Run a single experiment\"\"\"\n",
        "    experiment_id = experiment.get('experiment_id')\n",
        "    parameters = experiment.get('parameters', {})\n",
        "\n",
        "    # Extract denoising and guidance from the experiment parameters\n",
        "    denoising = parameters.get('denoising', 0.2)  # Default fallback\n",
        "    guidance = parameters.get('guidance', 0.2)    # Default fallback\n",
        "\n",
        "    # Prepare the data payload\n",
        "    data = {\n",
        "        \"start\": {\n",
        "            \"prompt\": \"\",\n",
        "            \"seed\": START_SEED,\n",
        "            \"denoising\": denoising,\n",
        "            \"guidance\": guidance\n",
        "        },\n",
        "        \"num_inference_steps\": STEPS,\n",
        "        \"seed_image_path\": SEED_IMAGE_PATH,\n",
        "        \"mask_image_path\": MASK_IMAGE_PATH,\n",
        "        \"alpha\": ALPHA,\n",
        "        \"end\": {\n",
        "            \"prompt\": \"\",\n",
        "            \"seed\": END_SEED,\n",
        "            \"denoising\": denoising,\n",
        "            \"guidance\": guidance\n",
        "        },\n",
        "        \"output_path\": f\"{OUTPUT_PATH}/experiment_{experiment_id}\",  # Unique output path per experiment\n",
        "    }\n",
        "\n",
        "    logger.info(f\"Running experiment {experiment_id} with denoising={denoising}, guidance={guidance}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"http://127.0.0.1:5000/run_inference/\", json=data)\n",
        "        logger.info(f\"Experiment {experiment_id} - Response status code: {response.status_code}\")\n",
        "        logger.info(f\"Experiment {experiment_id} - Response text: {response.text[:500]}\")\n",
        "\n",
        "        # Return experiment results\n",
        "        return {\n",
        "            \"experiment_id\": experiment_id,\n",
        "            \"parameters\": parameters,\n",
        "            \"status_code\": response.status_code,\n",
        "            \"response\": response.text[:500],\n",
        "            \"success\": response.status_code == 200\n",
        "        }\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Experiment {experiment_id} - Request failed: {e}\")\n",
        "        return {\n",
        "            \"experiment_id\": experiment_id,\n",
        "            \"parameters\": parameters,\n",
        "            \"error\": str(e),\n",
        "            \"success\": False\n",
        "        }"
      ],
      "metadata": {
        "id": "RMgyZ0cDpFP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate experiemental configs\n",
        "combinations = generate_simple_combinations()\n",
        "print(f\"generating {len(combinations)} simple experiments...\")"
      ],
      "metadata": {
        "id": "4hin2HctpFRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "run batch request\n",
        "\"\"\"\n",
        "\n",
        "# list of json results to be saved\n",
        "results = []\n",
        "\n",
        "for i, combo in enumerate(combinations):\n",
        "    print(f\"Experiment {i+1}/{len(combinations)}: denoising={combo['denoising']}, guidance={combo['guidance']}\")\n",
        "\n",
        "    request_data = create_simple_request_data(combo, i+1)\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            API_ENDPOINT,\n",
        "            json=request_data,\n",
        "            headers={'Content-Type': 'application/json'},\n",
        "            timeout=300\n",
        "        )\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            result = {\n",
        "                'experiment_id': i+1,\n",
        "                'parameters': combo,\n",
        "                'status': 'success',\n",
        "                'response': response.json()\n",
        "            }\n",
        "            print(f\"✓ Experiment {i+1} completed successfully\")\n",
        "        else:\n",
        "            result = {\n",
        "                'experiment_id': i+1,\n",
        "                'parameters': combo,\n",
        "                'status': 'error',\n",
        "                'error': f\"HTTP {response.status_code}: {response.text}\"\n",
        "            }\n",
        "            print(f\"✗ Experiment {i+1} failed: HTTP {response.status_code}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        result = {\n",
        "            'experiment_id': i+1,\n",
        "            'parameters': combo,\n",
        "            'status': 'error',\n",
        "            'error': str(e)\n",
        "        }\n",
        "        print(f\"✗ Experiment {i+1} failed: {e}\")\n",
        "\n",
        "    results.append(result)"
      ],
      "metadata": {
        "id": "gBrs1JmrpFTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save results\n",
        "save_results(results)"
      ],
      "metadata": {
        "id": "ww7nRenypY47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tyNjdlf1pY7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## batch decode"
      ],
      "metadata": {
        "id": "J9TBP6_5r62Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def decode_audio_from_json(json_file_path, output_wav_path):\n",
        "    \"\"\"\n",
        "    Decode audio from JSON response and save as WAV file.\n",
        "\n",
        "    Args:\n",
        "        json_file_path: Path to the JSON file containing the API response\n",
        "        output_wav_path: Path where to save the WAV file\n",
        "\n",
        "    Returns:\n",
        "        bool: True if successful, False otherwise\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the JSON file\n",
        "        with open(json_file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Extract the audio data (base64 encoded)\n",
        "        audio_base64 = data.get('audio', '')\n",
        "\n",
        "        if not audio_base64:\n",
        "            logger.warning(f\"No audio data found in {json_file_path}\")\n",
        "            return False\n",
        "\n",
        "        # Remove the data URL prefix if present\n",
        "        if audio_base64.startswith('data:audio/mpeg;base64,'):\n",
        "            audio_base64 = audio_base64.replace('data:audio/mpeg;base64,', '')\n",
        "        elif audio_base64.startswith('data:audio/wav;base64,'):\n",
        "            audio_base64 = audio_base64.replace('data:audio/wav;base64,', '')\n",
        "\n",
        "        # Decode base64 to binary\n",
        "        audio_binary = base64.b64decode(audio_base64)\n",
        "        logger.info(f\"Successfully decoded {len(audio_binary)} bytes of audio data from {json_file_path}\")\n",
        "\n",
        "        # Save as WAV file\n",
        "        with open(output_wav_path, 'wb') as f:\n",
        "            f.write(audio_binary)\n",
        "\n",
        "        logger.info(f\"Audio saved as: {output_wav_path}\")\n",
        "\n",
        "        # Also save the spectrogram image if present (optional)\n",
        "        image_base64 = data.get('image', '')\n",
        "        if image_base64:\n",
        "            if image_base64.startswith('data:image/jpeg;base64,'):\n",
        "                image_base64 = image_base64.replace('data:image/jpeg;base64,', '')\n",
        "            elif image_base64.startswith('data:image/png;base64,'):\n",
        "                image_base64 = image_base64.replace('data:image/png;base64,', '')\n",
        "\n",
        "            image_binary = base64.b64decode(image_base64)\n",
        "\n",
        "            # Uncomment if you want to save spectrograms\n",
        "            # image_path = output_wav_path.replace('.wav', '_spectrogram.jpg')\n",
        "            # with open(image_path, 'wb') as f:\n",
        "            #     f.write(image_binary)\n",
        "            # logger.info(f\"Spectrogram saved as: {image_path}\")\n",
        "\n",
        "        # Print duration if available\n",
        "        duration = data.get('duration_s', 0)\n",
        "        if duration > 0:\n",
        "            logger.info(f\"Audio duration: {duration:.2f} seconds\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        logger.error(f\"Error reading JSON file {json_file_path}: {e}\")\n",
        "        return False\n",
        "    except base64.binascii.Error as e:\n",
        "        logger.error(f\"Error decoding base64 data from {json_file_path}: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing {json_file_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "def batch_decode_audio_from_directory(directory_path):\n",
        "    \"\"\"\n",
        "    Process all JSON files in a directory and decode audio from each one.\n",
        "\n",
        "    Args:\n",
        "        directory_path: Path to the directory containing JSON files\n",
        "    \"\"\"\n",
        "    directory = Path(directory_path)\n",
        "\n",
        "    if not directory.exists():\n",
        "        logger.error(f\"Directory does not exist: {directory_path}\")\n",
        "        return\n",
        "\n",
        "    if not directory.is_dir():\n",
        "        logger.error(f\"Path is not a directory: {directory_path}\")\n",
        "        return\n",
        "\n",
        "    # Find all JSON files in the directory\n",
        "    json_files = list(directory.glob(\"*.json\"))\n",
        "\n",
        "    if not json_files:\n",
        "        logger.warning(f\"No JSON files found in directory: {directory_path}\")\n",
        "        return\n",
        "\n",
        "    logger.info(f\"Found {len(json_files)} JSON files to process\")\n",
        "\n",
        "    successful_conversions = 0\n",
        "    failed_conversions = 0\n",
        "\n",
        "    # Process each JSON file\n",
        "    for json_file in json_files:\n",
        "        logger.info(f\"Processing: {json_file.name}\")\n",
        "\n",
        "        # Generate output WAV path (same directory, same name but .wav extension)\n",
        "        wav_file = json_file.with_suffix('.wav')\n",
        "\n",
        "        # Skip if WAV file already exists (optional - remove this check if you want to overwrite)\n",
        "        if wav_file.exists():\n",
        "            logger.info(f\"WAV file already exists, skipping: {wav_file.name}\")\n",
        "            continue\n",
        "\n",
        "        # Decode the audio\n",
        "        success = decode_audio_from_json(json_file, wav_file)\n",
        "\n",
        "        if success:\n",
        "            successful_conversions += 1\n",
        "        else:\n",
        "            failed_conversions += 1\n",
        "\n",
        "        logger.info(\"-\" * 50)  # Separator for readability\n",
        "\n",
        "    # Summary\n",
        "    logger.info(f\"Batch processing completed!\")\n",
        "    logger.info(f\"Successfully converted: {successful_conversions} files\")\n",
        "    logger.info(f\"Failed conversions: {failed_conversions} files\")\n",
        "    logger.info(f\"Total processed: {len(json_files)} files\")"
      ],
      "metadata": {
        "id": "xdaejSzXr8yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main function to run the batch decoder\"\"\"\n",
        "    # CHANGE THIS TO YOUR DIRECTORY PATH\n",
        "    directory_path = \"/content/drive/MyDrive/Training-Free-StyleID/results/audio/\"\n",
        "\n",
        "    # You can also make it interactive\n",
        "    # directory_path = input(\"Enter the directory path containing JSON files: \").strip()\n",
        "\n",
        "    batch_decode_audio_from_directory(directory_path)"
      ],
      "metadata": {
        "id": "jUkjkanpr80e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "9n9oqH20sAMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uHv6Zl9MsAOF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}