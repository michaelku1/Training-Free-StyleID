{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d8a9937fd6f946beb9919c349156943c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4466350728df42d7a7da3a6e884754e1",
              "IPY_MODEL_2988748e279f476ca2d1c216bd3ce02b",
              "IPY_MODEL_992779c3ccdc4339a179a4d6f6ce424d"
            ],
            "layout": "IPY_MODEL_e226ed3365eb4f0c9308a662130c7ab6"
          }
        },
        "4466350728df42d7a7da3a6e884754e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27bbe165a2f242ffb562a930991abf22",
            "placeholder": "​",
            "style": "IPY_MODEL_d459aaba41674435bd444e90ab5c8ec5",
            "value": "100%"
          }
        },
        "2988748e279f476ca2d1c216bd3ce02b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea005ef028bc45f59e55fe16c70a600c",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4cdc8fbda07b4da487b3f3ba09143bac",
            "value": 10
          }
        },
        "992779c3ccdc4339a179a4d6f6ce424d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_427f98728693477d9a1ed67033b5026e",
            "placeholder": "​",
            "style": "IPY_MODEL_1d262e2d1d504bc4bd8a018c3cd3c884",
            "value": " 10/10 [00:09&lt;00:00,  1.35it/s]"
          }
        },
        "e226ed3365eb4f0c9308a662130c7ab6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27bbe165a2f242ffb562a930991abf22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d459aaba41674435bd444e90ab5c8ec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea005ef028bc45f59e55fe16c70a600c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cdc8fbda07b4da487b3f3ba09143bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "427f98728693477d9a1ed67033b5026e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d262e2d1d504bc4bd8a018c3cd3c884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# cd to project root\n",
        "%cd /content/drive/MyDrive/riffusion\n",
        "\n",
        "# install environment\n",
        "!curl -L https://repo.anaconda.com/miniconda/Miniconda3-py39_4.12.0-Linux-x86_64.sh -o miniconda.sh\n",
        "!chmod +x miniconda.sh\n",
        "!sh miniconda.sh -b -p /content/miniconda\n",
        "!/content/miniconda/bin/pip install -r requirements.txt\n",
        "!/content/miniconda/bin/pip install --upgrade ipython ipykernel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zCHSse7N_WVa",
        "outputId": "056e1dde-942f-44f3-977d-41018c812232"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/riffusion'\n",
            "/content\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 73.0M  100 73.0M    0     0   129M      0 --:--:-- --:--:-- --:--:--  129M\n",
            "PREFIX=/content/miniconda\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /content/miniconda\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - _openmp_mutex==4.5=1_gnu\n",
            "    - brotlipy==0.7.0=py39h27cfd23_1003\n",
            "    - ca-certificates==2022.3.29=h06a4308_1\n",
            "    - certifi==2021.10.8=py39h06a4308_2\n",
            "    - cffi==1.15.0=py39hd667e15_1\n",
            "    - charset-normalizer==2.0.4=pyhd3eb1b0_0\n",
            "    - colorama==0.4.4=pyhd3eb1b0_0\n",
            "    - conda-content-trust==0.1.1=pyhd3eb1b0_0\n",
            "    - conda-package-handling==1.8.1=py39h7f8727e_0\n",
            "    - conda==4.12.0=py39h06a4308_0\n",
            "    - cryptography==36.0.0=py39h9ce1e76_0\n",
            "    - idna==3.3=pyhd3eb1b0_0\n",
            "    - ld_impl_linux-64==2.35.1=h7274673_9\n",
            "    - libffi==3.3=he6710b0_2\n",
            "    - libgcc-ng==9.3.0=h5101ec6_17\n",
            "    - libgomp==9.3.0=h5101ec6_17\n",
            "    - libstdcxx-ng==9.3.0=hd4cf53a_17\n",
            "    - ncurses==6.3=h7f8727e_2\n",
            "    - openssl==1.1.1n=h7f8727e_0\n",
            "    - pip==21.2.4=py39h06a4308_0\n",
            "    - pycosat==0.6.3=py39h27cfd23_0\n",
            "    - pycparser==2.21=pyhd3eb1b0_0\n",
            "    - pyopenssl==22.0.0=pyhd3eb1b0_0\n",
            "    - pysocks==1.7.1=py39h06a4308_0\n",
            "    - python==3.9.12=h12debd9_0\n",
            "    - readline==8.1.2=h7f8727e_1\n",
            "    - requests==2.27.1=pyhd3eb1b0_0\n",
            "    - ruamel_yaml==0.15.100=py39h27cfd23_0\n",
            "    - setuptools==61.2.0=py39h06a4308_0\n",
            "    - six==1.16.0=pyhd3eb1b0_1\n",
            "    - sqlite==3.38.2=hc218d9a_0\n",
            "    - tk==8.6.11=h1ccaba5_0\n",
            "    - tqdm==4.63.0=pyhd3eb1b0_0\n",
            "    - tzdata==2022a=hda174b7_0\n",
            "    - urllib3==1.26.8=pyhd3eb1b0_0\n",
            "    - wheel==0.37.1=pyhd3eb1b0_0\n",
            "    - xz==5.2.5=h7b6447c_0\n",
            "    - yaml==0.2.5=h7b6447c_0\n",
            "    - zlib==1.2.12=h7f8727e_1\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-4.5-1_gnu\n",
            "  brotlipy           pkgs/main/linux-64::brotlipy-0.7.0-py39h27cfd23_1003\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2022.3.29-h06a4308_1\n",
            "  certifi            pkgs/main/linux-64::certifi-2021.10.8-py39h06a4308_2\n",
            "  cffi               pkgs/main/linux-64::cffi-1.15.0-py39hd667e15_1\n",
            "  charset-normalizer pkgs/main/noarch::charset-normalizer-2.0.4-pyhd3eb1b0_0\n",
            "  colorama           pkgs/main/noarch::colorama-0.4.4-pyhd3eb1b0_0\n",
            "  conda              pkgs/main/linux-64::conda-4.12.0-py39h06a4308_0\n",
            "  conda-content-tru~ pkgs/main/noarch::conda-content-trust-0.1.1-pyhd3eb1b0_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.8.1-py39h7f8727e_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-36.0.0-py39h9ce1e76_0\n",
            "  idna               pkgs/main/noarch::idna-3.3-pyhd3eb1b0_0\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.35.1-h7274673_9\n",
            "  libffi             pkgs/main/linux-64::libffi-3.3-he6710b0_2\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.3.0-h5101ec6_17\n",
            "  libgomp            pkgs/main/linux-64::libgomp-9.3.0-h5101ec6_17\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.3.0-hd4cf53a_17\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.3-h7f8727e_2\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1n-h7f8727e_0\n",
            "  pip                pkgs/main/linux-64::pip-21.2.4-py39h06a4308_0\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py39h27cfd23_0\n",
            "  pycparser          pkgs/main/noarch::pycparser-2.21-pyhd3eb1b0_0\n",
            "  pyopenssl          pkgs/main/noarch::pyopenssl-22.0.0-pyhd3eb1b0_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py39h06a4308_0\n",
            "  python             pkgs/main/linux-64::python-3.9.12-h12debd9_0\n",
            "  readline           pkgs/main/linux-64::readline-8.1.2-h7f8727e_1\n",
            "  requests           pkgs/main/noarch::requests-2.27.1-pyhd3eb1b0_0\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.100-py39h27cfd23_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-61.2.0-py39h06a4308_0\n",
            "  six                pkgs/main/noarch::six-1.16.0-pyhd3eb1b0_1\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.38.2-hc218d9a_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.11-h1ccaba5_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.63.0-pyhd3eb1b0_0\n",
            "  tzdata             pkgs/main/noarch::tzdata-2022a-hda174b7_0\n",
            "  urllib3            pkgs/main/noarch::urllib3-1.26.8-pyhd3eb1b0_0\n",
            "  wheel              pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.5-h7b6447c_0\n",
            "  yaml               pkgs/main/linux-64::yaml-0.2.5-h7b6447c_0\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.12-h7f8727e_1\n",
            "\n",
            "\n",
            "Preparing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /content/miniconda\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\n",
            "Collecting ipython\n",
            "  Downloading ipython-8.18.1-py3-none-any.whl (808 kB)\n",
            "\u001b[K     |████████████████████████████████| 808 kB 5.7 MB/s \n",
            "\u001b[?25hCollecting ipykernel\n",
            "  Downloading ipykernel-6.30.1-py3-none-any.whl (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 67.6 MB/s \n",
            "\u001b[?25hCollecting matplotlib-inline\n",
            "  Downloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\n",
            "Collecting exceptiongroup\n",
            "  Downloading exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
            "Collecting prompt-toolkit<3.1.0,>=3.0.41\n",
            "  Downloading prompt_toolkit-3.0.52-py3-none-any.whl (391 kB)\n",
            "\u001b[K     |████████████████████████████████| 391 kB 58.8 MB/s \n",
            "\u001b[?25hCollecting jedi>=0.16\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 57.8 MB/s \n",
            "\u001b[?25hCollecting pygments>=2.4.0\n",
            "  Downloading pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 61.3 MB/s \n",
            "\u001b[?25hCollecting decorator\n",
            "  Downloading decorator-5.2.1-py3-none-any.whl (9.2 kB)\n",
            "Collecting traitlets>=5\n",
            "  Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "\u001b[K     |████████████████████████████████| 44 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting stack-data\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Collecting pexpect>4.3\n",
            "  Downloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting pyzmq>=25\n",
            "  Downloading pyzmq-27.0.2-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (863 kB)\n",
            "\u001b[K     |████████████████████████████████| 863 kB 55.9 MB/s \n",
            "\u001b[?25hCollecting tornado>=6.2\n",
            "  Downloading tornado-6.5.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
            "\u001b[K     |████████████████████████████████| 443 kB 60.6 MB/s \n",
            "\u001b[?25hCollecting nest-asyncio>=1.4\n",
            "  Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
            "Collecting comm>=0.1.1\n",
            "  Downloading comm-0.2.3-py3-none-any.whl (7.3 kB)\n",
            "Collecting packaging>=22\n",
            "  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 6.2 MB/s \n",
            "\u001b[?25hCollecting debugpy>=1.6.5\n",
            "  Downloading debugpy-1.8.16-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 65.6 MB/s \n",
            "\u001b[?25hCollecting jupyter-client>=8.0.0\n",
            "  Downloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 99.3 MB/s \n",
            "\u001b[?25hCollecting psutil>=5.7\n",
            "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "\u001b[K     |████████████████████████████████| 277 kB 79.0 MB/s \n",
            "\u001b[?25hCollecting jupyter-core!=5.0.*,>=4.12\n",
            "  Downloading jupyter_core-5.8.1-py3-none-any.whl (28 kB)\n",
            "Collecting parso<0.9.0,>=0.8.4\n",
            "  Downloading parso-0.8.5-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 75.4 MB/s \n",
            "\u001b[?25hCollecting python-dateutil>=2.8.2\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 85.6 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata>=4.8.3\n",
            "  Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting zipp>=3.20\n",
            "  Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
            "Collecting platformdirs>=2.5\n",
            "  Downloading platformdirs-4.4.0-py3-none-any.whl (18 kB)\n",
            "Collecting ptyprocess>=0.5\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting wcwidth\n",
            "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: six>=1.5 in ./miniconda/lib/python3.9/site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel) (1.16.0)\n",
            "Collecting executing>=1.2.0\n",
            "  Downloading executing-2.2.1-py2.py3-none-any.whl (28 kB)\n",
            "Collecting pure-eval\n",
            "  Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
            "Collecting asttokens>=2.1.0\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: zipp, wcwidth, typing-extensions, traitlets, pure-eval, ptyprocess, platformdirs, parso, executing, asttokens, tornado, stack-data, pyzmq, python-dateutil, pygments, prompt-toolkit, pexpect, matplotlib-inline, jupyter-core, jedi, importlib-metadata, exceptiongroup, decorator, psutil, packaging, nest-asyncio, jupyter-client, ipython, debugpy, comm, ipykernel\n",
            "Successfully installed asttokens-3.0.0 comm-0.2.3 debugpy-1.8.16 decorator-5.2.1 exceptiongroup-1.3.0 executing-2.2.1 importlib-metadata-8.7.0 ipykernel-6.30.1 ipython-8.18.1 jedi-0.19.2 jupyter-client-8.6.3 jupyter-core-5.8.1 matplotlib-inline-0.1.7 nest-asyncio-1.6.0 packaging-25.0 parso-0.8.5 pexpect-4.9.0 platformdirs-4.4.0 prompt-toolkit-3.0.52 psutil-7.0.0 ptyprocess-0.7.0 pure-eval-0.2.3 pygments-2.19.2 python-dateutil-2.9.0.post0 pyzmq-27.0.2 stack-data-0.6.3 tornado-6.5.2 traitlets-5.14.3 typing-extensions-4.15.0 wcwidth-0.2.13 zipp-3.23.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install additional dependencies\n",
        "! pip install dacite flask_cors flask_ngrok pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MCDHSnk8_e4N",
        "outputId": "d9d5b939-e65b-42d4-fa39-d2115967a9c1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dacite\n",
            "  Downloading dacite-1.9.2-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting flask_cors\n",
            "  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting flask_ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: flask>=0.9 in /usr/local/lib/python3.12/dist-packages (from flask_cors) (3.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.12/dist-packages (from flask_cors) (3.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from flask_ngrok) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask_cors) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask_cors) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask_cors) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask_cors) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask_cors) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->flask_ngrok) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->flask_ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->flask_ngrok) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->flask_ngrok) (2025.8.3)\n",
            "Downloading dacite-1.9.2-py3-none-any.whl (16 kB)\n",
            "Downloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Downloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok, dacite, flask_ngrok, flask_cors\n",
            "Successfully installed dacite-1.9.2 flask_cors-6.0.1 flask_ngrok-0.0.25 pyngrok-7.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# where script saved under\n",
        "%cd /content/drive/MyDrive/Training-Free-StyleID"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiEohULoAZ21",
        "outputId": "ba011cb7-5be1-402d-fd26-421a075a0fff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Training-Free-StyleID\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Flask server that serves the riffusion model as an API.\n",
        "\"\"\"\n",
        "\n",
        "import dataclasses\n",
        "import io\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "import typing as T\n",
        "from pathlib import Path\n",
        "\n",
        "import dacite\n",
        "import flask\n",
        "import PIL\n",
        "import torch\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Fix CUDA linear algebra backend to avoid cusolver errors\n",
        "torch.backends.cuda.preferred_linalg_library('magma')\n",
        "\n",
        "# NOTE original riffusion pipeline\n",
        "from riffusion.riffusion_pipeline import RiffusionPipeline\n",
        "from riffusion.datatypes import InferenceInput, InferenceOutput\n",
        "\n",
        "from riffusion.spectrogram_image_converter import SpectrogramImageConverter\n",
        "from riffusion.spectrogram_params import SpectrogramParams\n",
        "\n",
        "from riffusion.util import base64_util\n",
        "\n",
        "# from flask_ngrok import run_with_ngrok\n",
        "NGROK_AUTH_TOKEN = \"32MmrpMI4sZN558sIugyRuhDgDg_5AdY64F9xihYgNZZfyHJL\"\n",
        "\n",
        "# Flask app with CORS\n",
        "app = flask.Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "\n",
        "# Create a logger object\n",
        "logger = logging.getLogger(\"my_server\")\n",
        "logger.setLevel(logging.DEBUG)\n",
        "\n",
        "# Log at the INFO level to both stdout and disk\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.FileHandler(\"server.log\"))\n",
        "\n",
        "# Create a file handler to write logs to a file\n",
        "file_handler = logging.FileHandler(\"server.log\")\n",
        "file_handler.setLevel(logging.DEBUG)\n",
        "\n",
        "# set format\n",
        "formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
        "\n",
        "# initalise file handler\n",
        "file_handler.setFormatter(formatter)\n",
        "\n",
        "# initalise console handler\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.INFO)\n",
        "console_handler.setFormatter(formatter)\n",
        "\n",
        "# Add handlers to the logger\n",
        "logger.addHandler(file_handler)\n",
        "\n",
        "# Global variable for the model pipeline\n",
        "PIPELINE: T.Optional[RiffusionPipeline] = None\n",
        "\n",
        "# set auth token for free n_grok usage\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "odngfjdy-fjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_request(\n",
        "    inputs: InferenceInput,\n",
        "    pipeline: RiffusionPipeline,\n",
        ") -> T.Union[str, T.Tuple[str, int]]:\n",
        "    \"\"\"\n",
        "    Does all the heavy lifting of the request.\n",
        "\n",
        "    Args:\n",
        "        inputs: The input dataclass\n",
        "        pipeline: The riffusion model pipeline\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the seed image by ID\n",
        "    init_image_path = Path(f\"{inputs.seed_image_path}.png\")\n",
        "\n",
        "    print(\"######################### input image path: \", init_image_path)\n",
        "\n",
        "    if not init_image_path.is_file():\n",
        "        return f\"Invalid seed image: {inputs.seed_image_path}\", 400\n",
        "    init_image = PIL.Image.open(str(init_image_path)).convert(\"RGB\")\n",
        "\n",
        "    # Load the mask image by ID\n",
        "    mask_image: T.Optional[PIL.Image.Image] = None\n",
        "\n",
        "    # NOTE pass mask image here\n",
        "    # mask_image = PIL.Image.open(\"...png\").convert(\"RGB\")\n",
        "    if inputs.mask_image_path:\n",
        "        mask_image_path = Path(f\"{inputs.mask_image_path}.png\")\n",
        "        if not mask_image_path.is_file():\n",
        "            return f\"Invalid mask image: {inputs.mask_image_path}\", 400\n",
        "        mask_image = PIL.Image.open(str(mask_image_path)).convert(\"RGB\")\n",
        "\n",
        "    print(\"inputs:\", inputs)\n",
        "    print(\"init_image\", init_image)\n",
        "    print(\"mask_image\", mask_image)\n",
        "\n",
        "    # Execute the model to get the spectrogram image\n",
        "    image = pipeline.riffuse(\n",
        "        inputs,\n",
        "        init_image=init_image,\n",
        "        mask_image=mask_image,\n",
        "    )\n",
        "\n",
        "    # TODO(hayk): Change the frequency range to [20, 20k] once the model is retrained\n",
        "    params = SpectrogramParams(\n",
        "        min_frequency=0,\n",
        "        max_frequency=10000,\n",
        "    )\n",
        "\n",
        "    # Reconstruct audio from the image\n",
        "    # TODO(hayk): It may help performance a bit to cache this object\n",
        "    # Use CPU for audio processing to avoid CUDA solver issues\n",
        "    converter = SpectrogramImageConverter(params=params, device=\"cpu\")\n",
        "\n",
        "    # NOTE 轉回 audio signal\n",
        "    segment = converter.audio_from_spectrogram_image(\n",
        "        image,\n",
        "        apply_filters=True,\n",
        "    )\n",
        "\n",
        "    # Export audio to MP3 bytes\n",
        "    mp3_bytes = io.BytesIO()\n",
        "    segment.export(mp3_bytes, format=\"mp3\")\n",
        "    mp3_bytes.seek(0)\n",
        "\n",
        "    # Export image to JPEG bytes\n",
        "    image_bytes = io.BytesIO()\n",
        "    image.save(image_bytes, exif=image.getexif(), format=\"JPEG\")\n",
        "    image_bytes.seek(0)\n",
        "\n",
        "    # Assemble the output dataclass\n",
        "    output = InferenceOutput(\n",
        "        image=\"data:image/jpeg;base64,\" + base64_util.encode(image_bytes),\n",
        "        audio=\"data:audio/mpeg;base64,\" + base64_util.encode(mp3_bytes),\n",
        "        duration_s=segment.duration_seconds,\n",
        "    )\n",
        "\n",
        "    # release memory\n",
        "    import gc\n",
        "    del image, mask_image, init_image  # delete big tensors\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()  # free cached memory\n",
        "    torch.cuda.ipc_collect()  # (optional) reclaim inter-process memory\n",
        "\n",
        "    output_name = f\"{''.join(inputs.seed_image_path.split('/')[-2:])}_to_{''.join(inputs.mask_image_path.split('/')[-2:])}\"\n",
        "\n",
        "    with open(f\"{inputs.output_path}/{output_name}.json\", \"w\") as f:\n",
        "        json.dump(dataclasses.asdict(output), f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"output json path:\", output_name, flush=True)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "HB1JxDpb_J7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Where built-in seed images are stored\n",
        "# import traceback\n",
        "# def run_app_background(*args, **kwargs):\n",
        "#     try:\n",
        "#         # Your existing Flask + ngrok code\n",
        "#         global PIPELINE\n",
        "\n",
        "#         import logging, sys\n",
        "#         logging.basicConfig(\n",
        "#             level=logging.DEBUG,\n",
        "#             format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "#             handlers=[logging.StreamHandler(sys.stdout)]\n",
        "#         )\n",
        "#         app.logger.setLevel(logging.DEBUG)\n",
        "\n",
        "#         app.logger.info(\"Loading RiffusionPipeline...\")\n",
        "#         PIPELINE = RiffusionPipeline.load_checkpoint(\n",
        "#             checkpoint=kwargs.get(\"checkpoint\", \"riffusion/riffusion-model-v1\"),\n",
        "#             use_traced_unet=not kwargs.get(\"no_traced_unet\", False),\n",
        "#             device=kwargs.get(\"device\", \"cuda\")\n",
        "#         )\n",
        "#         app.logger.info(\"Pipeline loaded successfully!\")\n",
        "\n",
        "#         public_url = ngrok.connect(kwargs.get(\"port\", 5000))\n",
        "#         print(f\" * ngrok tunnel URL: {public_url}\", flush=True)\n",
        "\n",
        "#         app.logger.info(f\"Starting Flask server on port {kwargs.get('port', 5000)}...\")\n",
        "#         app.run(port=kwargs.get(\"port\", 5000), debug=kwargs.get(\"debug\", True), use_reloader=False)\n",
        "\n",
        "#     except Exception:\n",
        "#         print(\"Exception in background thread:\", flush=True)\n",
        "#         traceback.print_exc()\n",
        "\n",
        "def run_app(\n",
        "    *,\n",
        "    checkpoint: str = \"riffusion/riffusion-model-v1\",\n",
        "    no_traced_unet: bool = False,\n",
        "    device: str = \"cuda\",\n",
        "    port: int = 5000,\n",
        "    debug: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Run a Flask API that serves the given riffusion model checkpoint\n",
        "    and exposes it via ngrok.\n",
        "    \"\"\"\n",
        "    global PIPELINE\n",
        "\n",
        "    # Initialize the model\n",
        "    PIPELINE = RiffusionPipeline.load_checkpoint(\n",
        "        checkpoint=checkpoint,\n",
        "        use_traced_unet=not no_traced_unet,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    # Set debug mode\n",
        "    app.debug = debug\n",
        "\n",
        "    # Start ngrok tunnel\n",
        "    public_url = ngrok.connect(port)\n",
        "    print(f\" * ngrok tunnel URL: {public_url}\", flush=True)\n",
        "\n",
        "    # Start Flask server\n",
        "    app.run(port=port)\n",
        "\n",
        "\n",
        "@app.route(\"/run_inference/\", methods=[\"POST\"])\n",
        "def run_inference():\n",
        "    \"\"\"\n",
        "    Execute the riffusion model as an API.\n",
        "\n",
        "    Inputs:\n",
        "        Serialized JSON of the InferenceInput dataclass\n",
        "\n",
        "    Returns:\n",
        "        Serialized JSON of the InferenceOutput dataclass\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Parse the payload as JSON\n",
        "    json_data = json.loads(flask.request.data)\n",
        "\n",
        "    # Log the request\n",
        "    logging.info(json_data)\n",
        "\n",
        "    # Parse an InferenceInput dataclass from the payload\n",
        "    try:\n",
        "        inputs = dacite.from_dict(InferenceInput, json_data)\n",
        "    except dacite.exceptions.WrongTypeError as exception:\n",
        "        logging.info(json_data)\n",
        "        return str(exception), 400\n",
        "    except dacite.exceptions.MissingValueError as exception:\n",
        "        logging.info(json_data)\n",
        "        return str(exception), 400\n",
        "\n",
        "    # NOTE\n",
        "    response = compute_request(\n",
        "        inputs=inputs,\n",
        "        pipeline=PIPELINE,\n",
        "    )\n",
        "\n",
        "    # Log the total time\n",
        "    logging.info(f\"Request took {time.time() - start_time:.2f} s\")\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "# @app.route(\"/run_inference/\", methods=[\"POST\"])\n",
        "# def run_inference():\n",
        "#     \"\"\"\n",
        "#     Execute the riffusion model as an API.\n",
        "\n",
        "#     Inputs:\n",
        "#         Serialized JSON of the InferenceInput dataclass\n",
        "\n",
        "#     Returns:\n",
        "#         Serialized JSON of the InferenceOutput dataclass\n",
        "#     \"\"\"\n",
        "#     start_time = time.time()\n",
        "\n",
        "#     # Parse the payload as JSON\n",
        "#     json_data = json.loads(flask.request.data)\n",
        "\n",
        "#     # Log the request\n",
        "#     logging.info(json_data)\n",
        "\n",
        "#     # Parse an InferenceInput dataclass from the payload\n",
        "#     try:\n",
        "#         inputs = dacite.from_dict(InferenceInput, json_data)\n",
        "#     except dacite.exceptions.WrongTypeError as exception:\n",
        "#         logging.info(json_data)\n",
        "#         return str(exception), 400\n",
        "#     except dacite.exceptions.MissingValueError as exception:\n",
        "#         logging.info(json_data)\n",
        "#         return str(exception), 400\n",
        "\n",
        "#     # NOTE\n",
        "#     response = compute_request(\n",
        "#         inputs=inputs,\n",
        "#         pipeline=PIPELINE,\n",
        "#     )\n",
        "\n",
        "#     # Log the total time\n",
        "#     logging.info(f\"Request took {time.time() - start_time:.2f} s\")\n",
        "\n",
        "#     return response\n",
        "\n",
        "def start_server():\n",
        "  run_app()"
      ],
      "metadata": {
        "id": "yPqYI5-a-Yuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set to background thread\n",
        "import threading\n",
        "import time\n",
        "threading.Thread(target=start_server, daemon=True).start()\n",
        "\n",
        "# Give server time to start\n",
        "time.sleep(5) # may need longer startup"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2Il160ltI8o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if the server is running (keep checking, if this does not show anything then it means model is still uploading)\n",
        "!lsof -i:5000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kab1VvGmhyKk",
        "outputId": "678332fc-8998-41d5-ea6e-83859e3b2056"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMMAND PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n",
            "python3 890 root   80u  IPv4  80359      0t0  TCP localhost:5000 (LISTEN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run inference\n",
        "CUDA_DEVICE=1\n",
        "START_SEED=42\n",
        "END_SEED=123\n",
        "DENOISING=0.2\n",
        "GUIDANCE=0\n",
        "ALPHA=0\n",
        "STEPS=50\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/Training-Free-StyleID/results/audio\"\n",
        "# SEED_IMAGE_PATH=\"/content/drive/MyDrive/Training-Free-StyleID/results/riffusion_seed_mask_images/accordian123/1\"\n",
        "MASK_IMAGE_PATH=\"/content/drive/MyDrive/Training-Free-StyleID/results/riffusion_seed_mask_images/EGDB_DI_1/chopper/1\"\n",
        "SEED_IMAGE_PATH = \"/content/drive/MyDrive/Training-Free-StyleID/results/riffusion_seed_mask_images/EGDB_DI_1/clean/2\"\n",
        "\n",
        "# Run curl command\n",
        "# !CUDA_VISIBLE_DEVICES=\"$CUDA_DEVICE\" curl -X POST http://127.0.0.1:5000/run_inference/ -H \"Content-Type: application/json\" -d '{\"start\":{\"prompt\":\"\",\"seed\":'\"$START_SEED\"',\"denoising\":'\"$DENOISING\"',\"guidance\":'\"$GUIDANCE\"'},\"num_inference_steps\":'\"$STEPS\"',\"seed_image_path\":\"'\"$SEED_IMAGE_PATH\"'\",\"mask_image_path\":\"'\"$MASK_IMAGE_PATH\"'\",\"alpha\":'\"$ALPHA\"',\"end\":{\"prompt\":\"\",\"seed\":'\"$END_SEED\"',\"denoising\":'\"$DENOISING\"',\"guidance\":'\"$GUIDANCE\"', \"output_path\": '\"$OUTPUT_PATH\"'}}'"
      ],
      "metadata": {
        "id": "tv8mgGuBgLWX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "data = {\n",
        "    \"start\": {\"prompt\": \"\", \"seed\": START_SEED, \"denoising\": DENOISING, \"guidance\": GUIDANCE},\n",
        "    \"num_inference_steps\": STEPS,\n",
        "    \"seed_image_path\": SEED_IMAGE_PATH,\n",
        "    \"mask_image_path\": MASK_IMAGE_PATH,\n",
        "    \"alpha\": ALPHA,\n",
        "    \"end\": {\"prompt\": \"\", \"seed\": END_SEED, \"denoising\": DENOISING, \"guidance\": GUIDANCE, \"output_path\": OUTPUT_PATH}\n",
        "}\n",
        "\n",
        "try:\n",
        "    response = requests.post(\"http://127.0.0.1:5000/run_inference/\", json=data)\n",
        "    logger.info(f\"Response status code: {response.status_code}\")\n",
        "    logger.info(f\"Response text: {response.text[:500]}\")  # limit output to first 500 chars\n",
        "except Exception as e:\n",
        "    logger.error(f\"Request failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712,
          "referenced_widgets": [
            "d8a9937fd6f946beb9919c349156943c",
            "4466350728df42d7a7da3a6e884754e1",
            "2988748e279f476ca2d1c216bd3ce02b",
            "992779c3ccdc4339a179a4d6f6ce424d",
            "e226ed3365eb4f0c9308a662130c7ab6",
            "27bbe165a2f242ffb562a930991abf22",
            "d459aaba41674435bd444e90ab5c8ec5",
            "ea005ef028bc45f59e55fe16c70a600c",
            "4cdc8fbda07b4da487b3f3ba09143bac",
            "427f98728693477d9a1ed67033b5026e",
            "1d262e2d1d504bc4bd8a018c3cd3c884"
          ]
        },
        "id": "qrftbiHlvvt4",
        "outputId": "701d98f6-4a03-46a9-ebe6-7cb85ed680d6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "######################### input image path:  /content/drive/MyDrive/Training-Free-StyleID/results/riffusion_seed_mask_images/EGDB_DI_1/clean/2.png\n",
            "inputs: InferenceInput(start=PromptInput(prompt='', seed=42, negative_prompt=None, denoising=0.2, guidance=0), end=PromptInput(prompt='', seed=123, negative_prompt=None, denoising=0.2, guidance=0), alpha=0, num_inference_steps=50, seed_image_path='/content/drive/MyDrive/Training-Free-StyleID/results/riffusion_seed_mask_images/EGDB_DI_1/clean/2', mask_image_path='/content/drive/MyDrive/Training-Free-StyleID/results/riffusion_seed_mask_images/EGDB_DI_1/chopper/1', output_path=None)\n",
            "init_image <PIL.Image.Image image mode=RGB size=1816x512 at 0x7CB3141E40E0>\n",
            "mask_image <PIL.Image.Image image mode=RGB size=1816x512 at 0x7CB32F9E0320>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8a9937fd6f946beb9919c349156943c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Exception on /run_inference/ [POST]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flask/app.py\", line 1511, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flask/app.py\", line 919, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flask_cors/extension.py\", line 176, in wrapped_function\n",
            "    return cors_after_request(app.make_response(f(*args, **kwargs)))\n",
            "                                                ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flask/app.py\", line 917, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flask/app.py\", line 902, in dispatch_request\n",
            "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3189065214.py\", line 96, in run_inference\n",
            "    response = compute_request(\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1100236328.py\", line 73, in compute_request\n",
            "    image=\"data:image/jpeg;base64,\" + base64_util.encode(image_bytes),\n",
            "                                      ^^^^^^^^^^^\n",
            "NameError: name 'base64_util' is not defined\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Sep/2025 06:00:31] \"\u001b[35m\u001b[1mPOST /run_inference/ HTTP/1.1\u001b[0m\" 500 -\n",
            "INFO:my_server:Response status code: 500\n",
            "INFO:my_server:Response text: <!doctype html>\n",
            "<html lang=en>\n",
            "<title>500 Internal Server Error</title>\n",
            "<h1>Internal Server Error</h1>\n",
            "<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## decode base64"
      ],
      "metadata": {
        "id": "HzJ_MTRpecrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import base64\n",
        "import io\n",
        "from pathlib import Path\n",
        "\n",
        "def decode_audio_from_json(json_file_path, output_wav_path):\n",
        "    \"\"\"\n",
        "    Decode audio from JSON response and save as WAV file.\n",
        "\n",
        "    Args:\n",
        "        json_file_path: Path to the JSON file containing the API response\n",
        "        output_wav_path: Path where to save the WAV file\n",
        "    \"\"\"\n",
        "    # Read the JSON file\n",
        "    with open(json_file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Extract the audio data (base64 encoded)\n",
        "    audio_base64 = data.get('audio', '')\n",
        "\n",
        "    if not audio_base64:\n",
        "        print(\"No audio data found in JSON response\")\n",
        "        return\n",
        "\n",
        "    # Remove the data URL prefix if present\n",
        "    if audio_base64.startswith('data:audio/mpeg;base64,'):\n",
        "        audio_base64 = audio_base64.replace('data:audio/mpeg;base64,', '')\n",
        "\n",
        "    # Decode base64 to binary\n",
        "    try:\n",
        "        audio_binary = base64.b64decode(audio_base64)\n",
        "        print(f\"Successfully decoded {len(audio_binary)} bytes of audio data\")\n",
        "\n",
        "        # Save as WAV file\n",
        "        with open(output_wav_path, 'wb') as f:\n",
        "            f.write(audio_binary)\n",
        "\n",
        "        print(f\"Audio saved as: {output_wav_path}\")\n",
        "\n",
        "        # Also save the spectrogram image if present\n",
        "        image_base64 = data.get('image', '')\n",
        "        if image_base64:\n",
        "            if image_base64.startswith('data:image/jpeg;base64,'):\n",
        "                image_base64 = image_base64.replace('data:image/jpeg;base64,', '')\n",
        "\n",
        "            image_binary = base64.b64decode(image_base64)\n",
        "\n",
        "            # NOTE save the generated output audio spectrogram image\n",
        "            # image_path = output_wav_path.replace('.wav', '_spectrogram.jpg')\n",
        "\n",
        "            # with open(image_path, 'wb') as f:\n",
        "            #     f.write(image_binary)\n",
        "\n",
        "            # print(f\"Spectrogram saved as: {image_path}\")\n",
        "\n",
        "        # Print duration\n",
        "        duration = data.get('duration_s', 0)\n",
        "        print(f\"Audio duration: {duration:.2f} seconds\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error decoding audio: {e}\")"
      ],
      "metadata": {
        "id": "mO_XnVcwefbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_file_path = \"\"\n",
        "output_wav_path = \"\"\n",
        "\n",
        "decode_audio_from_json(json_file_path, output_wav_path)"
      ],
      "metadata": {
        "id": "FQIThY8VefdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## try pyngrok"
      ],
      "metadata": {
        "id": "lyg6bs1CgMvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THI8NHW9gSUq",
        "outputId": "acde4a68-64dd-4786-b82b-ad2916b64161"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import typing as T\n",
        "import logging\n",
        "import time\n",
        "import json\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "import dacite\n",
        "\n",
        "app = Flask(__name__)\n",
        "PIPELINE = None  # will be initialized in run_app\n",
        "\n",
        "def run_app(\n",
        "    *,\n",
        "    checkpoint: str = \"riffusion/riffusion-model-v1\",\n",
        "    no_traced_unet: bool = False,\n",
        "    device: str = \"cuda\",\n",
        "    port: int = 5000,\n",
        "    debug: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Run a Flask API that serves the given riffusion model checkpoint\n",
        "    and exposes it via ngrok.\n",
        "    \"\"\"\n",
        "    global PIPELINE\n",
        "\n",
        "    # Initialize the model\n",
        "    PIPELINE = RiffusionPipeline.load_checkpoint(\n",
        "        checkpoint=checkpoint,\n",
        "        use_traced_unet=not no_traced_unet,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    # Set debug mode\n",
        "    app.debug = debug\n",
        "\n",
        "    # Start ngrok tunnel\n",
        "    public_url = ngrok.connect(port)\n",
        "    print(f\" * ngrok tunnel URL: {public_url}\")\n",
        "\n",
        "    # Start Flask server\n",
        "    app.run(port=port)\n",
        "\n",
        "def start_server():\n",
        "  run_app()"
      ],
      "metadata": {
        "id": "gb613_tygQHz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set to background thread\n",
        "import threading\n",
        "threading.Thread(target=start_server, daemon=True).start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atFteNCiroM5",
        "outputId": "c91b44f5-32a0-4ae3-96d3-8acb56fe6e1f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: Falling back to float32 on cpu, float16 is unsupported\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1977693823.py:27: UserWarning: WARNING: cuda is not available, using cpu instead.\n",
            "  PIPELINE = RiffusionPipeline.load_checkpoint(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test run_app() with n_grok setting"
      ],
      "metadata": {
        "id": "unm7QRbZX7yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test app server\n",
        "# from flask import Flask, request, jsonify\n",
        "# from flask_ngrok import run_with_ngrok\n",
        "# import argh\n",
        "\n",
        "# def run_app():\n",
        "#   app = Flask(__name__)\n",
        "#   run_with_ngrok(app)  # starts ngrok when app.run() is called\n",
        "\n",
        "#   @app.route(\"/hello\", methods=[\"GET\"])\n",
        "#   def hello():\n",
        "#       return jsonify({\"msg\": \"Hello from Flask in Colab!\"})\n",
        "\n",
        "#   @app.route(\"/echo\", methods=[\"POST\"])\n",
        "#   def echo():\n",
        "#       data = request.json\n",
        "#       return jsonify({\"you_sent\": data})\n",
        "\n",
        "#   app.run()\n",
        "\n",
        "# this line is the problem\n",
        "# argh.dispatch_command(run_app)"
      ],
      "metadata": {
        "id": "73PLrNKyJ1eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def run_app(\n",
        "#     *,\n",
        "#     checkpoint: str = \"riffusion/riffusion-model-v1\",\n",
        "#     no_traced_unet: bool = False,\n",
        "#     device: str = \"cuda\",\n",
        "#     host: str = \"127.0.0.1\",\n",
        "#     port: int = 8001,\n",
        "#     debug: bool = False,\n",
        "#     ssl_certificate: T.Optional[str] = None,\n",
        "#     ssl_key: T.Optional[str] = None,\n",
        "# ):\n",
        "    \"\"\"\n",
        "    Run a flask API that serves the given riffusion model checkpoint.\n",
        "    \"\"\"\n",
        "    # Initialize the model\n",
        "    # global PIPELINE\n",
        "\n",
        "    # PIPELINE = RiffusionPipeline.load_checkpoint(\n",
        "    #     checkpoint=checkpoint,\n",
        "    #     use_traced_unet=True,\n",
        "    #     device=device,\n",
        "    # )\n",
        "\n",
        "    # TypeError: run_with_ngrok.<locals>.new_run() got an unexpected keyword argument 'debug'\n",
        "    # args = dict(\n",
        "    #     # debug=debug,\n",
        "    #     # threaded=False,\n",
        "    #     host=host,\n",
        "    #     port=port,\n",
        "    # )\n",
        "\n",
        "    # if ssl_certificate:\n",
        "    #     assert ssl_key is not None\n",
        "    #     args[\"ssl_context\"] = (ssl_certificate, ssl_key)\n",
        "\n",
        "    # app.run(**args)  # type: ignore"
      ],
      "metadata": {
        "id": "JxiPqe04X92K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_app() # app.run(**args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "vF6Jkz3YYJPF",
        "outputId": "09516ca5-8ba9-4784-db6a-efbe7633e899"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "run_with_ngrok.<locals>.new_run() got an unexpected keyword argument 'host'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-718536340.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_app\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1491935548.py\u001b[0m in \u001b[0;36mrun_app\u001b[0;34m(checkpoint, no_traced_unet, device, host, port, debug, ssl_certificate, ssl_key)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ssl_context\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mssl_certificate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssl_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: run_with_ngrok.<locals>.new_run() got an unexpected keyword argument 'host'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "# Flask app with CORS\n",
        "app = flask.Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# run background thread (with daemon)\n",
        "run_with_ngrok(app)"
      ],
      "metadata": {
        "id": "htTBoWk7YgAW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8kVk9kQvaQ3k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}